---
##
## Common, cluster-level values
##

## Image Registry
global:
  registry: {{ cluster.imageRegistry | default(model.properties.cluster.properties.imageRegistry.default) }}
  registry1: "registry1-docker-io.repo.lab.pl.alcatel-lucent.com"
  registry2: "csf-docker-delivered.repo.lab.pl.alcatel-lucent.com"
  postheal: 0
  postrestore: 0
  jobtimeout: 600

lcm:
  heal:
    timeout: 300
  scale:
    timeout: 600

## RBAC enabled flag
{% if (rbac | default("")).enabled is defined %}
rbac_enabled: {{ rbac.enabled | default(model.properties.rbac.properties.enabled.default) }}
{% endif %}

## If cluster_name not specified, the release name will be used
{% if (cluster | default("")).name is defined and (cluster | default("")).name != "" %}
cluster_name: {{ cluster.name }}
{% endif %}

## Cluster Type is one of master-slave, master-master, galera, standalone
{% if (cluster | default("")).type is defined %}
cluster_type: {{ cluster.type | default(model.properties.cluster.properties.type.default) }}
{% endif %}

## Times (in minutes) to wait for other pods to come up during pod restart:
# Maximum time pod should wait for all other pods to become available
{% if (cluster | default("")).node_wait is defined %}
max_node_wait: {{ cluster.node_wait | default(model.properties.cluster.properties.node_wait.default) }}
{% endif %}
# Maximum time (in seconds) pod should wait for additional pods after quorum
# reached.  To continue immediately after quorum reached, set to 0.
{% if (cluster | default("")).quorum_wait is defined %}
quorum_node_wait: {{ cluster.quorum_wait | default(model.properties.cluster.properties.quorum_wait.default) }}
{% endif %}

## Speficies the type of anti-affinity for scheduling pods to nodes.
### If hard, pods cannot be scheduled together on nodes, if soft,
### best-effort to avoid sharing nodes will be done
{% if (cluster | default("")).node_antiaffinity is defined and cluster.node_antiaffinity %}
nodeAntiAffinity: hard
{% else %}
nodeAntiAffinity: soft
{% endif %}

## Values on how to expose services
services:
  ##
  ## MySQL service exposes the mysql database service
  ##
  mysql:
    ## If not set, will be <release>-mysql
    #name:
    ## By default, set to ClusterIP to expose database only within cluster
    ## Set as NodePort to expose database externally
    {%- if (exposure | default("")).allow_external is defined and exposure.allow_external %}
    type: NodePort
    {%- else %}
    type: ClusterIP
    {%- endif %}
    ## mariadb-exporter metrics port (used if mariadb.metrics is enabled)
    exporter_port: 9104

  ##
  ## MariaDB Master exposes the pod that is master
  ## (only if using MaxScale and geo_redundancy.enabled)
  ##
  mariadb_master:
    ## If not set, will be <release>-mariadb-master
    #name:
    ## Set as NodePort to expose master externally for replication among
    ## Datacenters
    type: NodePort

  ##
  ## Maxscale exposes the administrative REST API interface of Maxscale
  ## (only if using MaxScale)
  ## Enabling geo-redundancy will automatically set type to "NodePort".
  ##
  maxscale:
    ## If not set, will be <release>-maxscale
    #name:
    ## Set as NodePort to expose maxscale service externally
    type: ClusterIP
    port: 8989
    ## maxscale-exporter metrics port (used if maxscale.metrics is enabled)
    exporter_port: 9195

  ##
  ## Admin exposes the admin container DB (redis.io) interface to all
  ## cluster pods.  (only if not Simplex)
  ##
  admin:
    ## If not set, will be <release>-admin
    #name:
    type: ClusterIP

##
## Values specific to the MariaDB (server)
##
mariadb:
  image:
    name: "cmdb/mariadb"
    tag: {{ cluster.imageTag | default(model.properties.cluster.properties.imageTag.default) }}
    pullPolicy: IfNotPresent

  ## The number of MariaDB pods to create
  {%- if (cluster | default("")).db_node_count is defined %}
  count: {{ cluster.db_node_count | default(model.properties.cluster.properties.db_node_count.default) }}
  {%- endif %}

  ## If automatic recover of the database should be performed on pod restarts
  heuristic_recover: {{ cluster.heuristic_recover | default(model.properties.cluster.properties.heuristic_recover.default) }}

  ## Interval (in seconds) when clean-logs should be performed on Master to
  ## cleanup binlogs that have been replicated to all slaves
  clean_log_interval: 3600

  ## If server audit logging should be enabled
  {% if (cluster | default("")).audit_logging is defined %}
  audit_logging:
    enabled: {{ cluster.audit_logging.enabled | default(model.properties.cluster.properties.audit_logging.properties.enabled.default) }}
    events: {{ cluster.audit_logging.events | default(model.properties.cluster.properties.audit_logging.properties.events.default) }}
  {%- endif %}

  ## If TLS should be used for data in flight?
  use_tls: {{ cluster.use_tls | default(model.properties.cluster.properties.use_tls.default) }}
  ## Certificates not supported in ComPaaS currently
  #certificates:
  #  secret:
 
  ## Resource QOS (per MariaDB container)
  {%- if (resources | default("")).mariadb is defined %}
  resources:
    requests:
      {%- if resources.mariadb.memory is defined %}
      memory: {{ resources.mariadb.memory | default(model.properties.resources.properties.mariadb.properties.memory.default) }}
      {%- endif %}
      {%- if resources.mariadb.cpu is defined %}
      cpu: {{ resources.mariadb.cpu | default(model.properties.resources.properties.mariadb.properties.cpu.default) }}
      {%- endif %}
    limits:
      {%- if resources.mariadb.memory is defined %}
      memory: {{ resources.mariadb.memory | default(model.properties.resources.properties.mariadb.properties.memory.default) }}
      {%- endif %}
      {%- if resources.mariadb.cpu is defined %}
      cpu: {{ resources.mariadb.cpu | default(model.properties.resources.properties.mariadb.properties.cpu.default) }}
      {%- endif %}
  {%- endif %}

  ## Node labels for mariadb pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ 
  nodeAffinity:
    enabled: true
    key: is_worker
    value: true

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  {%- if (resources | default("")).mariadb is defined %}
  persistence:
    accessMode: ReadWriteOnce
    size: {{ resources.mariadb.volume_size | default(model.properties.resources.properties.mariadb.properties.volume_size.default) }}
    {%- if resources.mariadb.volume_class != "" %}
    storageClass: {{ resources.mariadb.volume_class | default(model.properties.resources.properties.mariadb.properties.volume_class.default) }}
    {%- endif %}
    resourcePolicy: delete
    preserve_pvc: {{ resources.mariadb.preserve_pvc | default(model.properties.resources.properties.mariadb.properties.preserve_pvc.default) }}
    backup:
      enabled: {{ resources.mariadb.backup_volume_enabled | default(model.properties.resources.properties.mariadb.properties.backup_volume_enabled.default) }}
      {%- if resources.mariadb.backup_volume_class != "" %}
      storageClass: {{ resources.mariadb.backup_volume_class | default(model.properties.resources.properties.mariadb.properties.backup_volume_class.default) }}
      {%- endif %}
      accessMode: ReadWriteOnce
      size: {{ resources.mariadb.backup_volume_size | default(model.properties.resources.properties.mariadb.properties.backup_volume_size.default) }}
      dir: /mariadb/backup
  {%- endif %}

## A base64 encoded password.
  ## If not provided a random password will be generated.
  {%- if (admin | default("")).admin_passwd is defined and (admin | default("")).admin_passwd != "" %}
  root_password: {{ admin.admin_passwd | default(model.properties.admin.properties.admin_passwd.default) }}
  {%- endif %}

  ## If root user should be allowed from all hosts
  {%- if (admin | default("")).allow_admin_from_all is defined %}
  allow_root_all: {{ admin.allow_admin_from_all | default(model.properties.admin.properties.allow_admin_from_all.default) }}
  {%- endif %}

  ## List of databases to be created
  {%- if (admin | default("")).database is defined and (admin | default("")).database != "" %}
  databases:
    - name: {{ admin.database }}
  {%- endif %}
  #databases:
  #  - name: mydb
  #    character_set: keybcs2
  #    collate: keybcs2_bin
  #  - name: anotherdb

  ## List of users to be created
  #users:
  #  - name: test
  #    # base64 encoded
  #    password:  YWJjMTIz
  #    host: "%"
  #    privilege: ALL
  #    object: "mydb.*"
  #    # if use_tls set, require SSL/X509 or not
  #    requires: "SSL"
  #    with: "GRANT OPTION"

  ## A customized mysqld.conf to import
  mysqld_site_conf: |-
  {%- if (admin | default("")).mysqld is defined and (admin | default("")).mysqld != "" %}

    {{ admin.mysqld }}
  {%- else %}

    [mysqld]
    userstat = on
  {%- endif %}

  ## Replication
  ## The replication username to use in the database (default: repl@b.c)
  repl_user: "repl@b.c"
  ## A base64 encoded password for the replication user in the database
  ## IMPORTANT: Multi-site config must use same repl user/password among sites
  {%- if (admin | default("")).repl_passwd is defined and (admin | default("")).repl_passwd != "" %}
  repl_user_password: {{ admin.repl_passwd | default(model.properties.admin.properties.repl_passwd.default) }}
  {%- endif %}
  ## Set repl_use_ssl to true if SSL shoudl be used for replication traffic
  repl_use_ssl: false

  ## metrics
  metrics:
    {% if (cluster | default("")).metrics is defined %}
    enabled: {{ cluster.metrics.enabled | default(model.properties.cluster.properties.metrics.properties.enabled.default) }}
    {%- else %}
    enabled: false
    {%- endif %}

    user: exporter
    ## Provide base64 encoded password. If empty random password will be generated.
    #metrics_password: 
    image:
      name: "prom/mysqld-exporter"
      {% if (cluster | default("")).metrics is defined %}
      tag: {{ cluster.metrics.mariadbExporterImageTag | default(model.properties.cluster.properties.metrics.properties.mariadbExporterImageTag.default) }}
      {%- endif %}
      pullPolicy: IfNotPresent

    annotations:
      prometheus.io/scrape: "true"

    ## Resource QOS (per MariaDB-Metrics container)
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 256Mi
        cpu: 250m

  ## Grafana dashboard
  dashboard:
    {% if (cluster | default("")).metrics is defined %}
    enabled: {{ cluster.metrics.dashboard | default(model.properties.cluster.properties.metrics.properties.dashboard.default) }}
    {%- else %}
    enabled: false
    {%- endif %}

##
## Values specific to the MaxScale (proxy)
##
maxscale:
  image:
    name: "cmdb/maxscale"
    tag: {{ cluster.imageTag | default(model.properties.cluster.properties.imageTag.default) }}
    pullPolicy: IfNotPresent

  ## The number of MariaDB pods to create
  {%- if (cluster | default("")).mxs_node_count is defined %}
  count: {{ cluster.mxs_node_count | default(model.properties.cluster.properties.mxs_node_count.default) }}
  {%- endif %}

  ## Master Switchover timeout when Master pod is being deleted.
  ## This is used on mariadb preStop hook (defaults to 30 seconds)
  masterSwitchoverTimeout: 30

  ## Resource QOS (per MaxScale container)
  {%- if (resources | default("")).maxscale is defined %}
  resources:
    requests:
      {%- if resources.maxscale.memory is defined %}
      memory: {{ resources.maxscale.memory | default(model.properties.resources.properties.maxscale.properties.memory.default) }}
      {%- endif %}
      {%- if resources.maxscale.cpu is defined %}
      cpu: {{ resources.maxscale.cpu | default(model.properties.resources.properties.maxscale.properties.cpu.default) }}
      {%- endif %}
    limits:
      {%- if resources.maxscale.memory is defined %}
      memory: {{ resources.maxscale.memory | default(model.properties.resources.properties.maxscale.properties.memory.default) }}
      {%- endif %}
      {%- if resources.maxscale.cpu is defined %}
      cpu: {{ resources.maxscale.cpu | default(model.properties.resources.properties.maxscale.properties.cpu.default) }}
      {%- endif %}
  {%- endif %}

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  tolerations:
    - key: 'is_edge'
      operator: 'Equal'
      value: 'true'
      effect: 'NoExecute'

  ## Node labels for maxscale pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ 
  nodeAffinity:
    enabled: true
    {%- if (cluster | default("")).maxscale_node is defined %}
    key: is_{{ cluster.maxscale_node | default(model.properties.cluster.properties.maxscale_node.default) }}
    {%- else %}
    key: is_edge
    {%- endif %}
    value: true

  ## The maxscale username to use in the database
  maxscale_user: maxscale
  ## A base64 encoded password for the maxscale user in the database
  ## IMPORTANT: Multi-site config must use same repl user/password among sites
  {%- if (admin | default("")).mxs_passwd is defined and (admin | default("")).mxs_passwd != "" %}
  maxscale_user_password: {{ admin.mxs_passwd | default(model.properties.admin.properties.mxs_passwd.default) }}
  {%- endif %}

  ## Maxscale optional listeners/service ports to be configured
  ## The default listener/service is the ReadWriteSplit listener:
  ##   rwSplit    - writes to master, reads distributed to slaves (3306)
  ## Additionally the following listener/services can be configured:
  ##   readOnly   - read-only access load-balanced between all slaves (3307)
  ##   masterOnly - read-write access directed only to current Master (3308)
  ##   maxInfo    - metrics scrape port if maxscale metrics is enabled (8003)
  ##                (will be auto-enabled if maxscale.metrics is true)
  ## The listener can be disabled by commenting out or setting port to '0'.
  listeners:
    rwSplit: 3306
    readOnly: 3307
    {%- if (admin | default("")).maxscale_master_listener is defined and admin.maxscale_master_listener %}
    masterOnly: 3308
    {%- endif %}
    #maxInfo: 8003

  ## A customized maxscale configuration to import
  maxscale_site_conf: |-
  {%- if (admin | default("")).maxscale is defined and (admin | default("")).maxscale != "" %}

    {{ admin.maxscale }}
  {%- else %}

    [maxscale]
    threads = auto
    query_retries = 2
    query_retry_timeout = 10
  {%- endif %}

  ##
  ## Lists of SQL to inject at certain MaxScale events
  ##
## Not supported in ComPaaS yet
##
#  sql:
#
#    {%- if (admin | default("")).promote_sql is defined %}
#    ## Mariadb Node promoted to master
#    promotion: 
#      {{ admin.promote_sql }}
#    {%- endif %}
#
#    {%- if (admin | default("")).demote_sql is defined %}
#    ## Mariadb Node demoted to slave
#    demotion:
#      {{ admin.demote_sql }}
#    {%- endif %}

  ## leader-elector
  elector:
    image:
      name: "osdb/leader-elector"
      tag: {{ cluster.leaderElectorImageTag | default(model.properties.cluster.properties.leaderElectorImageTag.default) }}
      pullPolicy: IfNotPresent

    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 256Mi
        cpu: 250m

  ## metrics
  metrics:
    {% if (cluster | default("")).metrics is defined %}
    enabled: {{ cluster.metrics.enabled | default(model.properties.cluster.properties.metrics.properties.enabled.default) }}
    {%- else %}
    enabled: false
    {%- endif %}

    image:
      name: "cmdb/maxscale-exporter"
      {% if (cluster | default("")).metrics is defined %}
      tag: {{ cluster.metrics.maxscaleExporterImageTag | default(model.properties.cluster.properties.metrics.properties.maxscaleExporterImageTag.default) }}
      {%- endif %}
      pullPolicy: IfNotPresent

    annotations:
      prometheus.io/scrape: "true"

    ## Resource QOS (per MaxScale-Metrics container)
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 256Mi
        cpu: 250m

##
## Geo-redundancy replication values
## (Requires MaxScale, e.g. maxscale.count > 0)
##
{% if geo_redundancy is defined %}
geo_redundancy:
  ## Only enable geo_redundancy for multiple datacenters
  {%- if geo_redundancy.site_count is defined and geo_redundancy.site_count > 1 %}
  enabled: true
  {%- else %}
  enabled: false
  {%- endif %}

  ## Multi-site - Values used to setup replication among sites
  ## The number of sites (currently only 1 or 2 supported)
  ## The (1-based) index of this site - each site must have unique index
  site_index: {{ (geo_redundancy | default("")).site_index | default(model.properties.geo_redundancy.properties.site_index.default) }}

  ## Threshold (seconds) at which to alarm on lag between datacenters
  #lag_threshold:

  ## Interval (minutes) at which to purge slave logs
  #slave_purge_interval:

  ##
  ## Remote Datacenter definitions
  ##
  #remote:
  #  ## Name to associate with remote datacenter
  #  name: remote
  #
  #  ## MaxScale node at remote datacenter
  #  maxscale:
  #
  #  ## Master MariaDB node at remote datacenter
  #  master:
  #
  ## mariadb-master-remote ClusterIP for replicating to remote DC
  #  #master_remote_service_ip:
{% endif %}

##
## Values specific to the CMDB Administrative container
## (used for lifecycle and administrative Jobs)
##
admin:
  image:
    name: "cmdb/admin"
    tag: {{ cluster.imageTag | default(model.properties.cluster.properties.imageTag.default) }}
    pullPolicy: IfNotPresent

  ## Resource QOS (per Admin container)
  {%- if (resources | default("")).admin is defined %}
  resources:
    requests:
      {%- if resources.admin.memory is defined %}
      memory: {{ resources.admin.memory | default(model.properties.resources.properties.admin.properties.memory.default) }}
      {%- endif %}
      {%- if resources.admin.cpu is defined %}
      cpu: {{ resources.admin.cpu | default(model.properties.resources.properties.admin.properties.cpu.default) }}
      {%- endif %}
    limits:
      {%- if resources.admin.memory is defined %}
      memory: {{ resources.admin.memory | default(model.properties.resources.properties.admin.properties.memory.default) }}
      {%- endif %}
      {%- if resources.admin.cpu is defined %}
      cpu: {{ resources.admin.cpu | default(model.properties.resources.properties.admin.properties.cpu.default) }}
      {%- endif %}
  {%- endif %}

  ## Node labels for admin pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeAffinity:
    enabled: true
    key: is_worker
    value: true

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  {%- if (resources | default("")).admin is defined %}
  persistence:
    enabled: {{ resources.admin.volume_enabled | default(model.properties.resources.properties.admin.properties.volume_enabled.default) }}
    accessMode: ReadWriteOnce
    size: {{ resources.admin.volume_size | default(model.properties.resources.properties.admin.properties.volume_size.default) }}
    {%- if resources.admin.volume_class != "" %}
    storageClass: {{ resources.admin.volume_class | default(model.properties.resources.properties.admin.properties.volume_class.default) }}
    {%- endif %}
    resourcePolicy: delete
  {%- endif %}

  ## A recovery flag.  If changed, will trigger a heal of the database to occur
  {% if (lcm | default("")).heal_recovery_tag is defined %}
  recovery: {{ lcm.heal_recovery_tag | default(model.properties.lcm.properties.heal_recovery_tag.default) }}
  {% else %}
  recovery: "none"
  {% endif %}

  ## Set to true to change update config behavior to cause statefulset update
  ## (resulting in rolling pod restarts) when mariadb and maxscale configuration  ## is changed.  Default behavior is to not affect statefulset and services
  ## will be restarted within the pod.
  {% if (lcm | default("")).config_annotation is defined %}
  configAnnotation: {{ lcm.config_annotation | default(model.properties.lcm.properties.config_annotation.default) }}
  {% else %}
  configAnnotation: false 
  {% endif %}
 
  ## Should auto-heal be enabled?
  ## This will enable automated heal operations on cluster (Galera supported).
  ## the pauseDelay is how long (seconds) to wait to re-enable auto-heal after
  ## the cluster comes [back] up.
  autoHeal:
    enabled: true
    pauseDelay: 900

  ## Quick installation method
  {% if (lcm | default("")).quick_install is defined %}
  quickInstall: {{ lcm.quick_install | default(model.properties.lcm.properties.quick_install.default) }}
  {% else %}
  quickInstall: "no"
  {% endif %}

  ## If set, administrative jobs will be more verbose to stdout (kubectl logs)
  debug: {{ lcm and lcm.debug | default(model.properties.lcm.properties.debug.default) }}
  ## If debug is true, you can add a termination delay to all of the pre/port
  ## job hooks as follows (time in seconds).  To terminate early, create a
  ## /tmp/exit file in the container [with exit value if want to override].
  #jobDelay: 0

  ## Specify timeout (in seconds) for each job execution.  If job timeout
  ## occurs. the job will fail, however the job process will run to completion.
  preInstallTimeout: 120
  postInstallTimeout: 900
  preUpgradeTimeout: 180
  postUpgradeTimeout: 1800
  preDeleteTimeout: 120
  postDeleteTimeout: 180

  ## The activeDeadlineSeconds applies to the duration of the job, no matter
  ## how many Pods are created.  Once a Job reaches activeDeadlineSeconds, the
  ## Job and all of its Pods are terminated.  The result is that the job has a
  ## status with reason: DeadlineExceeded.  It's used in pre-upgrade hook to
  ## prevent too many failed pods being started in failure cases.
  activeDeadlineSeconds: 120 

hooks:
  ## Exposes the hook-delete-policy.  By default, this is set to delete the
  ## hooks only upon success.  In helm v2.9+, this should be set to
  ## before-hook-creation.  This can also be unset to avoid hook deletion
  ## for troubleshooting and debugging purposes
  deletePolicy: "hook-succeeded"

###---------------------------
### CBUR Parameters
###---------------------------

cbur:
  enabled: {{ (cbur | default("")).enabled | default(model.properties.cbur.properties.enabled.default) }}
  image:
    name: "cbur/cbura"
    tag: {{ (cbur | default("")).imageTag | default(model.properties.cbur.properties.imageTag.default) }}
    pullPolicy: "IfNotPresent"

  ## Exposes jobhookenable to control BrPolicy behaviour for galera
  ## When enabled, no action in postRestoreCmd and postrestore job hook
  ## for helm restore should run. When disabled, postRestoreCmd is executed 
  ## and subsequent out-of-band helm heal is required. Interim solution for CSFS-12078,
  ## set to disabled in ComPaas.
  jobhookenable: false
  ## defines the backup storage options, i.e. local, NetBackup, S3, Avamar.
  backendMode: local
  ## specifies if the backup scheduling cron job should be auto-scheduled
  autoEnableCron: true
  ## allows user to schedule backups
  cronSpec: "0 0 * * *"
  ## should backups be encrypted? `
  dataEncryption: true
  ## defines how many backup copies should be saved
  maxiCopy : 5

  resources:
    requests:
      memory: 256Mi
      cpu: 250m
    limits:
      memory: 256Mi
      cpu: 250m

