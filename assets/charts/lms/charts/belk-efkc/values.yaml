### global registry
global:
 # This registry is used for belk images
 registry: bcmt-registry:5000/corepaas-docker-local.bhisoj70.apac.nsn-net.net
 # This registry is used for cbur and kubectl images
 registry1: bcmt-registry:5000/corepaas-docker-local.bhisoj70.apac.nsn-net.net
 # This registry is used for alpine-curl image
 registry2: bcmt-registry:5000/corepaas-docker-local.bhisoj70.apac.nsn-net.net
 seccompAllowedProfileNames: docker/default
 seccompDefaultProfileName: docker/default
 ##For bare metal deployments, local storage shall be used as elastic search persistence, thus, "local-storage" shall be set in efkStorage. Otherwise you can use "cinder" for cloud environments. glusterfs is not supported.
 efkStorage: "glusterfs"

## Enable or disable components of umbrella chart for EFKC.
tags:
  belk-fluentd: true
  belk-elasticsearch: true
  belk-kibana: true
  belk-curator: true


## If searchguard is enabled then create base64 encoding for keystore, keystorepasswd, truststore, truststorepasswd file and provide to below variables.
## base64 encoding for passwords use below command
## echo -n <string in double quotes> | base64
## base64 encoding for files  use below command
## base64 <filename>  | tr -d '\n'
## paste the output of each base64 converted file, password to respective field without any double quotes.
## Example- To convert plain text password to base64 run this command
## echo -n "changeit" | base64
## Y2hhbmdlaXQ=
## Example- To convert root-ca.pem to base64 run this command
## base64 <root-ca.pem>  | tr -d '\n'
## LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURrRENDQW5pZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREJaTVIwd0d3WUtDWkltaVpQeUxHUUIKR1JZTlpXeGhjM1JwWTNObFlYSmphREVYTUJVR0NnbVNKb21UOGl4a0FSa1dCMnh2WjJkcGJtY3hIekFkQmdOVgpCQU1NRm1Wc1lYTjBhV056WldGeVkyZ3VJRkp2YjNRZ1EwRXdIaGNOTVRnd05qQTJNRGd6TmpRd1doY05Namd3Ck5qQTFNRGd6TmpRd1dqQlpNUjB3R3dZS0NaSW1pWlB5TEdRQkdSWU5aV3hoYzNScFkzTmxZWEpqYURFWE1CVUcKQ2dtU0pvbVQ4aXhrQVJrV0IyeHZaMmRwYm1jeEh6QWRCZ05WQkFNTUZtVnNZWE4wYVdOelpXRnlZMmd1SUZKdgpiM1FnUTBFd2dnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUNjM2dUNi9uWWRheDlSClpoZmllempiK1lIOURza20yc2ZjNUFhN0FZcDYxdlpBNlhoNG1NQVZ5MXF2Ylk1R0djSkhmbGxpbEo4Z0NVQ3EKN201Mks4K05uZ0pBdjlJZHhHNkp6RXkwdmN4cUZ4Qk9wOGtRcnFzd1hFVmNWcWV3Z1drRmhXWXBCWFAxVE94ZwpOcmJKaEozZ1B5RlZmUkZySTJNZXVGL21OcUh6S2Mwd0h2RjBTaUNWWEMwb28vTjUzYVg1c00vR3NIZzlrRDRhCjA2Nm5UNDVkUlMvTCtMaVlRR1I3WGFsdlNYejl2eW9UTDhvQllCWnRjVGhLaCtUekY2TU1TWEt5dDRhS2lXTk4KUFVEeEJGbG9SSzREeUtsekdadTViYUYyb1FRbGZvTXM1QkdhOW92aWpRWGlTenRqalZwUSt2UklaSXJJZTErUAp1Znk4UEVNUkFnTUJBQUdqWXpCaE1BNEdBMVVkRHdFQi93UUVBd0lCQmpBUEJnTlZIUk1CQWY4RUJUQURBUUgvCk1CMEdBMVVkRGdRV0JCUjNENU1UbFl6YzdMWkFmc2VUY1dESWxSY0ZSekFmQmdOVkhTTUVHREFXZ0JSM0Q1TVQKbFl6YzdMWkFmc2VUY1dESWxSY0ZSekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBYW02dlpFcGhja0JzQVl5ZQplZGFrNGZSeHhaNXJZL2FyWGRYdDBrcDJ0RmswU00zcHYya3dLaDZheWJBc3hwaU1wQ1FEWnE2M2hRNGN6RnRSCjlmTlpYMVF1ZlNUdDltbC9XK0dxaVpUUUVzTmp0TFBVVnQwTW5Sd1JTdGNHcWk1amZ3WGJaUzJ0dW85STN4cWQKbkREMXJYbWVoZW1VNTBoUHdybGhuNUY0OWYxZU5jaUpwbm8wc1BST2RveXgwYmk1RzgvWDNpT200M25tSDFxcgp4cVkvdHJLVDVWMTEzdUFQb0J6d3lJSGdLSFlIUXR4NzM0YVJLL3ZiYXpjZFc3S1UxR1NiNGxlM3VQdXdoV3luCjhTRmdnQ2pGV055RXV3V3l6eEdHdUF1Vm9HYjNkKyt2UmZWR2Njb1J3bGF1bEJuUTFwTW9NQktXbVJoY2RkckMKMWtFVzJBPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=


## To overwrite child helm chart values by parent define:
## chart_name:
##   parameters in values.yaml in child chart

## Overwrite values for fluentd
belk-fluentd:
  fluentd:
    resources:
      limits:
        cpu: "4"
        memory: "4000Mi"
      requests:
        cpu: "400m"
        memory: "300Mi"

#set the system name and system ID for non-container log messages
    EnvVars:
      system: "BCMT"
      systemId: "BCMT ID"
#enable_root_privilege: false is supported only when fluentd kind is StatefulSet or Deployment.
#if set to true fluentd container will run as root user and reads container, journal logs.
#setting it to false will run container as td-agent user and it will not read container, journal logs.
#Example- If kind is StatefulSet and you want to store the logs coming from kafka and you dont want to read any container/journal logs then set this flag to false.
#Donot change the flag unless required.
    enable_root_privilege: true
    # We are supporting three different type of kinds i.e DaemonSet, Deployment and StatefulSet
    kind: StatefulSet
    # When docker_selinux is enabled on BCMT, to read /var/log/messages, set privileged as True in securityContext
    securityContext:
       privileged: False
#set enabled as true if you want to add any certificate in fluentd configuration or if you want to use ssl for fluentd configuration while using SG.
#all the fluentd certificates will be present in "/etc/td-agent/certs"
#Make sure to use the same path in fluentd configuration with proper certificate names.
#Give certificate name and paste the base64 converted certificates or passwords in data section.
    fluentd_certificates:
      enabled: false
      data:
        #example
        #prometheus-crt.pem: <base64_crt_pem>
        #prometheus-key.pem: <base64_key_pem>
        #prometheus-root-ca.pem: <base64_root_ca_pem>
        #es-root-ca.pem: <base64_es_root_ca_pem>

# set the enabled value to true if some service to be exposed from fluentd like fluentd-promethues-plugin which exports fluentd metrics so that prometheus can scrap the metrics via this service and port
    service:
      enabled: true
      # if you want to provide your own name for service then provide the value in "custom_name"
      # Default value is template {{ "fullname" . }}
      # Delete the old chart and deploy new chart if you want to configure "custom_name" parameter.
      custom_name: ""  
      # type of service: None, ClusterIP
      type: ClusterIP
      # source port for forwarder
      port: 24224
      # source port for http
      port: 9000
      # metricsPort is for getting fluentd prometheus metrics.
      # 24231 is the default port of fluentd-prometheus-plugin.
      # If metricsPort is changed, update same port in fluentd-prometheus configuration in the respective .conf file and in the prometheus annotation below as well.
      metricsPort: 24231
      annotations:
        prometheus.io/port: "24231"
        prometheus.io/scrape: "true"
    
    # set to false only in case of fluentd running as StatefulSet or Deployment if you dont want to mount these directories.
    volume_mount_enable: true
    # fluentd volumes
    volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: dockercontainers
      hostPath:
        path: /data0/docker/

    # volume mounts
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: dockercontainers
      mountPath: /data0/docker/

    ## Node labels for pod assignment
    ### ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}

    ## Toleration is asking the K8S schedule to ignore a taint
    ### ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## Pod scheduling preferences.
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}
    
    persistence:
      #For 18.8 BCMT onwards storageClassName cinder will bydefault get picked up. For using the chart in BCMT-18.6 provide storageClassName which you have created.
      ##If you want to use local storage as volume give "local-storage" in storageClassName.
      storageClassName: ""
      accessMode: ReadWriteOnce
      size: 10Gi
      #set pvc_auto_delete to true if you want the persistent volume to also get deleted on deletion of the release.
      #This will delete all the previsous data stored in the persistent volume.
      #When local storage is used it will deleted only PVC not PV.
      pvc_auto_delete: false
     
    #configurable values are belk, clog-json, clog-journal, custom-value.
    #If you use belk configuration, it will read journal and container logs. journal log index pattern will be created as journal-<timestamp>. Container logs index pattern will be created as <namespace>-<timestamp> and logs are not segreated based on type of logs.
    #For harmonized logs use clog configuration, any data which does not follow harmonized logging format will be stored in index <namespace>-legacy
    #If you use clog-journal it will read journal logs with  index created as <namespace>-<type>.
    #If you use clog-json it will read container log with  index created as <namespace>-<type>. 
    #If you want to make any changes to the default configuration available(belk/clog), then go to charts/belk-fluentd/fluentd-config/ directory and make changes to respective .conf file.
    fluentd_config: custom-value
           
    configFile: |
      #If you have own configuration for fluentd other than provided by belk/clog then set fluentd_config: custom-value and provide your configuration below. Example-
      <system>
        workers 8
      </system>
      <source>
        @type http
        port 9000
        bind 0.0.0.0
      </source>
      
      <filter pmdata>
        @type record_transformer
        enable_ruby
        <record>
          date ${ require 'date'; DateTime.rfc3339(record["measurement_time"]).strftime('%Y-%m-%d') }
        </record>
      </filter>
      
      <filter fmdata>
        @type record_transformer
        enable_ruby
        <record>
          date ${ require 'date'; DateTime.rfc3339(record["alarm_time"]).strftime('%Y-%m-%d') }
        </record>
      </filter>
      
      <filter {*security_logs*,*debug_logs*,*LI_logs*,*audit_logs*}>
        @type record_transformer
        enable_ruby
        <record>
          date ${ require 'date'; DateTime.rfc3339(record["log_event_time_stamp"]).strftime('%Y-%m-%d') }
        </record>
      </filter>
      
      <match pmdata>
      @type copy
      <store>
      <buffer tag, date, vnf_name>
        @type file
        path  /tmp/fluentdlogs/pm
        timekey      1d
        flush_thread_count 8
        chunk_limit_size 4MB
        overflow_action drop_oldest_chunk
        flush_mode interval
        flush_interval 5s
      </buffer>
        @type elasticsearch
        index_name ${tag}-${vnf_name}-${date}
        type_name pm_data
        host elasticsearch
        port 9200
        logstash_format false
        compression_level best_compression
        bulk_message_request_threshold 40M
      </store>
      </match>
      
      <match fmdata>
      @type copy
      <store>
      <buffer tag, date, vnf_name>
        @type file
        path  /tmp/fluentdlogs/fm
        timekey      1d
        flush_thread_count 8
        chunk_limit_size 4MB
        overflow_action drop_oldest_chunk
        flush_mode interval
        flush_interval 5s
      </buffer>
        @type elasticsearch
        index_name ${tag}-${vnf_name}-${date}
        type_name fm_data
        host elasticsearch
        port 9200
        logstash_format false
        bulk_message_request_threshold 40M
      </store>
      </match>
      
      
      <match {*security_logs*,*debug_logs*,*LI_logs*,*audit_logs*}>
      @type copy
      <store>
      <buffer tag, date, facility, vnf_name>
        @type file
        path  /tmp/fluentdlogs/logs
        timekey      1d
        flush_thread_count 8
        chunk_limit_size 4MB
        overflow_action drop_oldest_chunk
        flush_mode interval
        flush_interval 5s
      </buffer>
        @type elasticsearch
        index_name ${tag}-${facility}-${vnf_name}-${date}
        type_name logs_data
        host elasticsearch
        port 9200
        logstash_format false
        bulk_message_request_threshold 40M
      </store>
      </match>

  ##Following parameters are added for configmap specific restore. Enable below flag if you need to restore fluentd configmap via helm restore
  cbur:
    enabled: false
    #the maximum copy you want to save.
    maxCopy: 5
    #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
    backendMode: "local"
    #It is used for scheduled backup task
    cronJob: "0 23 * * *"
    #Set below parameters to true for auto enabling cron job
    autoEnableCron: false

## Overwrite values for elasticsearch
belk-elasticsearch:
  elasticsearch_master:
    replicas: 3
    resources:
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    es_java_opts: "-Xms1g -Xmx1g"
    discovery_service: "elasticsearch-discovery"
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
    initialDelaySeconds: 30
    periodSeconds: 10

  elasticsearch_client:
    replicas: 3
    resources:
      limits:
        cpu: "1"
        memory: "4Gi"
      requests:
        cpu: "500m"
        memory: "2Gi"
    es_java_opts: "-Xms2g -Xmx2g"
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    #If SG is enabled you may have to increase the initialDelaySeconds depending on your cluster
    initialDelaySeconds: 90
    periodSeconds: 20

  esdata:
    replicas: 2
    resources:
      limits:
        cpu: "5"
        memory: "16Gi"
      requests:
        cpu: "500m"
        memory: "2Gi"
    es_java_opts: "-Xms8g -Xmx8g"
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
  persistence:
    enabled: true
    #Supported storageClasses for BELK data are cinder, local-storage, hostpath.
    #The value "" picks up the default storageClass configured in BCMT cluster.
    #If you want to use local storage as volume give "local-storage" in storageClassName.
    storageClassName: "efk"
    accessMode: ReadWriteOnce
# Size of persistent storage of data pod to store the elasticsearch data.
    size: 50Gi
    # Size of persistent storage for master pod to persist cluster state
    masterStorage: 1Gi
# set auto_delete to true when the PV also has to be deleted on deletion of the release.
# this will delete all the previous data stored in the persistent volume.
# When local storage is used only PVC will get deleted not PV.
    auto_delete: false

  #network_host is set to _site_ by default. To know more about network_host and configure this parameter, please refer https://www.elastic.co/guide/en/elasticsearch/reference/7.0/modules-network.html#network-interface-values
  #For IPv6 environment, this can be set to "_global:ipv6_". If the network interface is known, you can set it to "_[networkInterface]:ipv6_". For ex: "_eth0:ipv6_"
  network_host: _site_

# Backup and restore supports only with Cinder adn GlusterFS
  backup_restore:
    ## For production servers this number should likely be much larger.
    size: 25Gi

  cbur:
    enabled: false
    #an integer. This value only applies to statefulset. The value can be 0,1 or 2.
    #Recommended value of brOption for BELK is 0.
    brOption: 0
    #the maximum copy you want to save.
    maxCopy: 5
    #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
    backendMode: "local"
    #It is used for scheduled backup task
    cronJob: "0 23 * * *"
    #Set below parameters to true for auto enabling cron job
    autoEnableCron: false
    cbura:
      imageRepo: cbur/cbura
      imageTag: 1.0.3-983
      imagePullPolicy: IfNotPresent
      userId: 1000
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
      #tmp_size is the mounted volume size of /tmp directory for cbur-sidecar.
      #the value should be around double the size of backup_restore.size
      tmp_size: 50Gi

# One can deploy mulitple elasticsearch in same namespace with different elasticsearch service names.
  service:
    name: "elasticsearch"
    #Set prometheus_metrics to true to scrape metrics from elasticsearch
    prometheus_metrics:
      enabled: false
      #If searchguard is enabled, you will have to create a custom scrape job in cpro chart. Refer belk user-guide for the same.
      #Prometheus annotation for scraping metrics from elaticsearch https endpoints. If this annotation is modified, make sure to add the same name in custom scrape job created in cpro chart.
      pro_annotation_https_scrape: "prometheus.io/scrape_es"

  searchguard:
    enable: false
    #if authentication is required via keycloak, then set keycloak_auth to true and provide base64 format of keycloak rootCA under base64_keycloak_rootca_pem
    keycloak_auth: false
    base64_keycloak_rootca_pem: <base64 format of keycloak rootCA pem>
    user_name: admin
    # Create base64 encoding for keystore, keystorepasswd, truststore, truststorepasswd file and provide to below variables
    # base64 encoding for passwords use below command
    # echo -n <string in double quotes> | base64
    # base64 encoding for files  use below command
    # base64 <filename>  | tr -d '\n'
    keystore_type: JKS
    truststore_type: JKS
    base64Keystore: <base64_keystore.jks>
    base64KeystorePasswd: <base64_ks_pwd>
    base64Truststore: <base64_truststore.jks>
    base64TruststorePasswd: <base64_ts_pwd>
    base64ClientKeystore: <base64_admin-keystore.jks>
    base64RootCA: <base64_root-ca.pem>
    base64_client_cert: <base64-client-cert>
    base64_client_key: <base64-client-key>
    auth_admin_identity: <CN=admin,C=ELK>
    sg_configmap:
      sg_internal_users_yml: |-
        ---
        _sg_meta:
          type: "internalusers"
          config_version: 2
        admin:
          reserved: true
          hidden: false
          hash: "$2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG"
          backend_roles:
          - "admin"

      # refer https://docs.search-guard.com/latest/action-groups for details about built-in action groups
      sg_action_groups_yml: |-
        ---
        _sg_meta:
          type: "actiongroups"
          config_version: 2

      sg_config_yml: |-
        ---
        _sg_meta:
          type: "config"
          config_version: 2
        sg_config:
          dynamic:
            http:
              anonymous_auth_enabled: false
              xff:
                enabled: false
                internalProxies: '.+'

            authc:
              basic_internal_auth_domain:
                http_enabled: true
                transport_enabled: true
                order: 0
                http_authenticator:
                  type: "basic"
                  challenge: true   # Set this to false when keycloak authentication is enabled
                authentication_backend:
                  type: "intern"
              keycloak_auth_domain:
                http_enabled: false  # Set to true to enable keycloak authentication
                transport_enabled: true
                order: 1
                http_authenticator:
                  type: keycloak
                  challenge: false
                  config:
                    username_key: preferred_username
                    roles_key: <roles>
                    keycloak_connect_url: https://<keycloak_ip>:<port>/auth/realms/<realm_name>/.well-known/openid-configuration
                    client_id: <client_id>
                    client_secret: <client_secret>
                authentication_backend:
                    type: noop
              proxy_auth_domain:
                http_enabled: false
                transport_enabled: false
                order: 3
                http_authenticator:
                  type: "proxy"
                  challenge: false
                  config:
                    user_header: "x-proxy-user"
                    #roles_header: "x-proxy-roles"
                authentication_backend:
                  type: "noop"
              clientcert_auth_domain:
                http_enabled: false
                transport_enabled: false
                order: 2
                http_authenticator:
                  challenge: false
                  type: "clientcert"
                  config:
                    username_attribute: "cn"
                authentication_backend:
                  type: "noop"

      # refer https://docs.search-guard.com/latest/roles-permissions for configuring the roles.
      # Define your roles in this section.
      sg_roles_yml: |-
        ---
        _sg_meta:
          type: "roles"
          config_version: 2

      # refer https://docs.search-guard.com/latest/role-mapping-modes
      # Define rolesmapping in this section.
      sg_roles_mapping_yml: |-
        ---
        _sg_meta:
          type: "rolesmapping"
          config_version: 2
        SGS_ALL_ACCESS:
          reserved: true
          hidden: false
          backend_roles:
          - "admin"
          description: "Migrated from v6"

        SGS_OWN_INDEX:
          reserved: false
          hidden: false
          users:
          - "*"
        SGS_KIBANA_USER:
          reserved: false
          backend_roles:
          - "kibanauser"
          description: "Maps kibanauser to SGS_KIBANA_USER"
        SGS_READALL:
          reserved: true
          backend_roles:
          - "readall"
        SGS_KIBANA_SERVER:
          reserved: true
          users:
          - "kibanaserver"


## Overwrite values for kibana
#Follow BELK userguide to create kibana server certificates
belk-kibana:
  searchguard:
    enable: false
    #if authentication is required via keycloak, then set keycloak_auth to true and provide base64 format of keycloak rootCA under base64_keycloak_rootca_pem
    keycloak_auth: false
    base64_keycloak_rootca_pem: <base64_keycloak_rootCA_pem>
    base64_kib_es_username: <base64_kibana_username>
    base64_kib_es_password: <base64_kibana_pwd>
    base64_ES_RootCA: <base64-elasticsearch_root-ca.pem>
    base64ServerCrt: <base64-kibana-crt>
    base64ServerKey: <base64-kibana-key>
    kibana:
      es_ssl_verification_mode: certificate
      # provide kibana other ssl properties here and mount the properties in sslsecretvolume section.
      # secret_server_ssl_cert: <server crt key in the secret object>
  ## in case of sane with keycloak uncomment below section and provide required correct parameters
  #sane:
    #keycloak_admin_user_name: <base64_keycloak_admin_username>
    #keycloak_admin_password: <base64_keycloak_admin_password>
    #keycloak_sane_user_password: <base64_default_password_for_saneuser>
  kibana:
    resources:
      limits:
        cpu: "1000m"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    initialDelaySeconds: 150
    configMaps:
      kibana_configmap_yml: |-
        ---
        # Donot change sever name and host. This is default configuration.
        server.name: kibana
        server.customResponseHeaders: { "X-Frame-Options": "DENY" }
        #Enable server.ssl.supportedProtocols when SG is enabled.
        #server.ssl.supportedProtocols: ["TLSv1.2"]
        #searchguard cookie can be secured by setting the below parameter to true. Uncomment it when SG is enabled.
        #searchguard.cookie.secure: true
        # Whitelist basic headers and multi tenancy header
        ##elasticsearch.requestHeadersWhitelist: [ "Authorization", "sgtenant", "x-forwarded-for", "x-proxy-user", "x-proxy-roles" ]

        
        # uncomment below section for keycloak authentication and provide required correct parameters
        #searchguard.auth.type: "openid"
        #searchguard.openid.connect_url: "https://<keyclaok-ip>:<port>/auth/realms/<realm-name>/.well-known/openid-configuration"
        #searchguard.openid.client_id: "<client-id>"
        #searchguard.openid.client_secret: "<client-secret>"
        #searchguard.openid.header: "Authorization"
         ### for kibana service on ingress port is not required
        #searchguard.openid.base_redirect_url: "https://<kibana-ip>:<port>"
         ### Do not change root_ca file path as this is the default mount path.
        #searchguard.openid.root_ca: "/etc/kibana/certs/keycloak-root-ca.pem"

        # Uncomment below section for sane sso and provide required correct parameters
        #csan.enabled: "true"
        #csan.ssoproxy.url: https://<csan-ssoproxy-service-hostname>:<port>
        #searchguard.auth.unauthenticated_routes: ["/api/status", "/csan/v1/sso"]

    env:
      # if searchguard enabled use https instead of http. 
      # value is http://<elasticsearch_service_name>.<namespace>:9200.
      # Namespace is required only when elasticsearch and kibana are in different namespace
      # if SG is enabled then uncomment SSL and certificate parameters. Do not change the certificate names. 
      - name: "ELASTICSEARCH_HOSTS"
        value: "http://elasticsearch:9200"
      #- name: "SERVER_SSL_ENABLED"
      #  value: "true"
      #- name: "SERVER_SSL_CERTIFICATE"
      #  value: "/etc/kibana/certs/kibana.crt.pem"
      #- name: "SERVER_SSL_KEY"
      #  value: "/etc/kibana/certs/kibana.key.pem"

    ## Node labels for pod assignment
    #### ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}
    
    ## Toleration is asking the K8S schedule to ignore a taint
    #### ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## Pod scheduling preferences.
    ### ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}
    
       
  ingress:
    enabled: false
    # If SG is enabled then uncomment 3 annotations. i.e ingress.class, ssl-passthrough, secure-backends.
    annotations:
      ingress.citm.nokia.com/sticky-route-services: $cookie_JSESSIONID|JSESSIONID ip_cookie
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      #kubernetes.io/ingress.class: nginx
      #nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      #nginx.ingress.kubernetes.io/secure-backends: "true"

    ## Kibana Ingress hostnames
    ## May be provided if Ingress is enabled
    #host: "*"
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  service:
    #if you are deploying more than one kibana in the same namespace change the service name
    name: kibana
    # to access kibana service via NodePort set service.type to NodePort and set ingress.enable parameter to false
    type: "NodePort"
  kibanabaseurl:
    url: /
    #Do not change cg(capture group) parameter below unless you want to change/modify nginx rewrite-target for kibana ingress
    cg: "/?(.*)"

  ##Following parameters are added for configmap specific restore. Enable below flag if you need to restore kibana configmap via helm restore
  cbur:
    enabled: false
    #the maximum copy you want to save.
    maxCopy: 5
    #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
    backendMode: "local"
    #It is used for scheduled backup task
    cronJob: "0 23 * * *"
    #Set below parameters to true for auto enabling cron job
    autoEnableCron: false

## Overwrite values for curator
belk-curator:
  searchguard:
    enable: false
    base64_ca_certificate: <base64_root-ca.pem>

  curator:
    resources:
      limits:
        cpu: "120m"
        memory: "120Mi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    schedule: "0 1 * * *"
    configMaps:
      # Delete indices older than 7 days
      action_file_yml: |-
        ---
        actions:
          1:
            action: delete_indices
            description: "Delete indices older than 7 (based on index name)"
            options:
              timeout_override:
              continue_if_exception: False
              disable_action: False
              ignore_empty_list: True
            filters:
            - filtertype: age
              source: name
              direction: older
              timestring: '%Y.%m.%d'
              unit: days
              unit_count: 7
          ##When cbur is enabled for elasticsearch,following section can be uncommented to configure deletion of old snapshots.
          ## Curator cronjob should be scheduled to run after backup cronjob.
          #2:
          #  action: delete_snapshots
          #  description: "Delete snapshot older than 5 days (based on index name)"
          #  options:
          #    repository: es_backup
          #    ignore_empty_list: True
          #  filters:
          #  - filtertype: age
          #    source: name
          #    direction: older
          #    timestring: '%Y.%m.%d'
          #    unit: days
          #    unit_count: 5
      # Having config_yaml WILL override the other config
      config_yml: |-
        ---
        client:
          # communicating elasticsearch via SG certificates
          certificate: '/etc/elasticsearch-curator/certs/root-ca.pem'
          #hosts is the elasticsearch service name and namespace is where the elasticsearch is deployed.
          hosts:
          #<elasticsearch_service_name.namespace>
          - elasticsearch
          #elasticsearch username and password.
          http_auth: 'admin:admin'
          master_only: 'false'
          port: 9200
          # ssl_no_validate property should be 'false' when you are using SG
          ssl_no_validate: 'true'
          timeout: '60'
          url_prefix: ''
          # use_ssl property should be true when you are using SG.
          use_ssl: 'false'
        logging:
          blacklist:
          - elasticsearch
          - urllib3
          logfile: ''
          logformat: default
          loglevel: INFO 


