<source>
  @type systemd
  path /var/log/journal
  tag journal
  <storage>
    @type local
    path /var/log/td-agent/journal.pos
    persistent true
  </storage>
</source>

<source>
  @type tail
  path /var/log/messages,/var/log/bcmt/apiserver/audit.log
  pos_file /var/log/td-agent/containers.json.access.pos
  tag kubernetes.non-container
  #read_from_head true
  <parse>
      @type regexp
      expression /(^(?<message>[^\{].+))|(^(?<header>[^\{]+)?(?<log>\{.+\})$)/
  </parse>
</source>

<match kubernetes.non-container>
  @type rewrite_tag_filter
  <rule>
    key header
    pattern \sjournal:( \[stdout\])*\s*$
    tag kubernetes.non-container.clear
  </rule>
  <rule>
    key message
    pattern ^\{.+\}$
    tag kubernetes.non-container.json
  </rule>
  <rule>
    key log
    pattern .+
    tag kubernetes.non-container.legacy
  </rule>
</match>

<match kubernetes.non-container.clear>
  @type null
</match>

<filter kubernetes.non-container.legacy>
  @type record_modifier
  <record>
    log ${{"message":record['log']}}
    type log
  </record>
</filter>

<filter kubernetes.non-container.json>
    @type parser
    key_name message
    format json
    time_parse false
</filter>

<filter kubernetes.non-container.*>
  @type record_modifier
  remove_keys _dummy1_,_dummy2_,_dummy3_
  <record>
    _dummy1_ ${if !record.has_key?("host"); record["host"] = ENV["HOST"]; end; "UNAVAILABLE"}
    _dummy2_ ${if !record.has_key?("system"); record["system"] = ENV["SYSTEM"]; end; "UNAVAILABLE"}
    _dummy3_ ${if !record.has_key?("systemid"); record["systemid"] = ENV["SYSTEMID"]; end; "UNAVAILABLE"}
  </record>
</filter>

<match journal>
  @type rewrite_tag_filter
  <rule>
  key CONTAINER_NAME
  pattern ^k8s_
  tag kubernetes.journal.container
  </rule>
</match>

<filter kubernetes.journal.container>
  @type kubernetes_metadata
  kubernetes_url https://kubernetes.default.svc:443
  use_journal true
  de_dot false
  #@log_level trace
</filter>

<filter kubernetes.journal.container>
  @type systemd_entry
  field_map {"CONTAINER_ID": "container"}
  fields_lowercase true
</filter>

<filter kubernetes.**>
  @type record_transformer
  enable_ruby true
  <record>
    time ${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
  </record>
</filter>

#this filter is used for C API which remove "[stdout]" from log
#if CLOG Unified Logging C API won't be used, this filter can be removed
<filter kubernetes.journal.container>
  @type parser
  format /^(\[stdout\])*(?<message>.+)$/
  key_name message
  suppress_parse_error_log true
  reserve_data yes
</filter>

<match kubernetes.journal.container>
  @type rewrite_tag_filter
  <rule>
    key message
    pattern ^\s*\{(.+,)?\s*"type":.+\}\s*$
    tag nokia.logging.json
  </rule>
  <rule>
    key message
    pattern .+
    tag nokia.logging.legacy
  </rule>
</match>

<filter nokia.logging.json>
  @type parser
  key_name message
  reserve_data true
  format json
  time_parse false
</filter>

<match nokia.logging.json>
  @type rewrite_tag_filter
  <rule>
    key "type"
    pattern ^(.+)$
    tag "nokia.logging.$1"
  </rule>
</match>

##Here is a sample for fluent-plugin-brevity-control
##this plugin can be used to remove reduplicative records
##for more details, please refer to CLOG User Guide
#<filter nokia.logging.log>
#  @type brevity_control
#  interval 10
#  num 2
#  attr_keys log.message, level
#  max_slot_num 100000
#  stats_msg_fields kubernetes
#</filter>

<filter nokia.logging.*>
  @type record_modifier
  remove_keys _dummy1_
  <record>
    _dummy1_ ${record.has_key?("kubernetes") ? record["namespace"]=record["kubernetes"]["namespace_name"]:record["namespace"] = 'UNAVAILABLE'}
    tenant_msgtype ${record["namespace"]}.${record["type"]}
  </record>
</filter>

<filter nokia.logging.legacy>
  @type record_modifier
  <record>
    log ${{"message":record["message"]}}
    type log
  </record>
</filter>

<match kubernetes.non-container.json>
  @type rewrite_tag_filter
  <rule>
    key type
    pattern ^(.+)$
    tag nokia.logging.default.$1
  </rule>
</match>

<match kubernetes.non-container.legacy>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern ^(.+)$
    tag nokia.logging.default.legacy
  </rule>
</match>

<filter nokia.logging.**>
  @type clog
</filter>

<match nokia.logging.*>
  @type rewrite_tag_filter
  <rule>
    key tenant_msgtype
    pattern ^(.+)\.log$
    tag nokia.logging.$1.tmp
  </rule>
  <rule>
    key tenant_msgtype
    pattern ^(.+\..+)$
    tag nokia.logging.$1
  </rule>
  <rule>
    key tenant_msgtype
    pattern ^(.+)\.$
    tag nokia.logging.$1.legacy
  </rule>
</match>

<match nokia.logging.*.tmp>
    @type rewrite_tag_filter
    <rule>
      key facility
      pattern 24
      tag nokia.logging.${tag_parts[2]}.authlog
    </rule>
    <rule>
      key message
      pattern ^\s*\{(.+)?\s*"event-type":.+\}\s*$
      tag nokia.logging.${tag_parts[2]}.auditlog
    </rule>
    <rule>
        key message
        pattern ^\s*\{(.+)?\s*"message":.+\}\s*$
        tag nokia.logging.${tag_parts[2]}.log
    </rule>
    <rule>
        key message
        pattern .+
        tag nokia.logging.${tag_parts[2]}.log2
    </rule>
</match>

<filter nokia.logging.*.*>
  @type record_modifier
  remove_keys priority, _uid, _gid, _systemd_slice, _machine_id, _transport, _cap_effective, _comm, _exe, _cmdline, _hostname, _systemd_cgroup, _systemd_unit, _selinux_context, _boot_id, _pid, message, container_id_full, container_name, container_tag, _source_realtime_timestamp, docker, kubernetes, tenant_msgtype
</filter>

#############################
## prometheus config start ##
#############################
<source>
@type prometheus
#uncomment below line "bin" in IPv6 Env
#bind ::
</source>

<filter nokia.logging.*.counter>
  @type record_modifier
  <record>
      counter_value ${record["counter"]["value"]}
      counter_mid ${record["counter"]["mid"]}
      counter_object ${record["counter"]["object"]}
      counter_id ${record["counter"]["id"]}
  </record>
</filter>

#
# WARNING: Don't use the match directive here. Otherwise, the messages of counter
# cannot be processed further by other parts behind this block.
#
<filter nokia.logging.*.counter>
  @type prometheus
  <metric>
      name meas_gauge
      type gauge
      desc "measurement exported via fluent-plugin-prometheus"
      key counter_value
      append_timestamp true
      metric_expiration 300
      group_keys mid, host, namespace
      audit_interval 30
      <labels>
          mid ${counter_mid}
          object ${counter_object}
          id ${counter_id}
          host ${host}
          namespace ${namespace}
      </labels>
  </metric>
</filter>

#############################
## prometheus config end   ##
#############################

<match nokia.logging.*.*>
    @type relabel
    @label @NOKIA-LOGGING-ROUTING
</match>

<label @NOKIA-LOGGING-ROUTING>
    <filter nokia.logging.*.alarm>
        @type record_transformer
        <record>
            @class com.nsn.cam.alma.own.api.event.UnifiedLoggingAlarmEvent
        </record>
    </filter>

#
# tag: nokia.logging.${namespace}.${type}
#
    <match nokia.logging.*.alarm>
      @type copy
    #######################
    ## amqp config start ##
    #######################
       # <store>
          # @type amqp
          # host rabbitmq_service:5672
          # vhost /
          # user user
          # pass password
          # key event
          # exchange cfw
          # exchange_type direct
          # content_type application/json
          # durable true
          # <buffer>
              # @type file
              # path /var/log/td-agent/rabbitmq-buffer/nokia.logging.all.alarm
              # flush_mode immediate
              # overflow_action block
              # total_limit_size 1024m
          # </buffer>
      # </store>
    #####################
    ## amqp config end ##
    #####################

    ########################
    ## kafka config start ##
    ########################
      # <store>
          # @type kafka_buffered
          # output_data_type json
          # buffer_type file
          # ssl_ca_certs_from_system false
          # get_kafka_client_log false
          # max_send_retries 2
          # buffer_path /var/log/td-agent/kafka-buffer/nokia.logging.all.alarm
          # brokers kafka_server:9200
          # default_topic fluentd-topic
          # flush_interval 3s
      # </store>
    ######################
    ## kafka config end ##
    ######################

      <store>
          @type rewrite_tag_filter
          <rule>
            key type
            pattern .+
            tag ${tag}.es
          </rule>
      </store>
    </match>

    <match nokia.logging.**>
      @type copy
      <store>
          @type elasticsearch_dynamic
          host elasticsearch
          port 9200
          resurrect_after 5s

          type_name fluentd
          time_key time
          utc_index true
          time_key_exclude_timestamp true
          logstash_format true
          logstash_prefix fluentd-${tag_parts[2]}-${tag_parts[3]}

          #Below commented lines are ssl properties. Please uncomment below commented lines when you want to use.
          #user admin
          #password admin
          #scheme https
          #ssl_verify false
          #ssl_version TLSv1_2
          <buffer tag, time, namespace, type>
              @type file
              path /var/log/td-agent/elasticsearch-buffer/nokia.logging.all.all
              flush_mode interval
              flush_interval 30s
              timekey 3600
              retry_forever true
              retry_max_interval 5s
              overflow_action block
              total_limit_size 1024m
          </buffer>
      </store>
    </match>
</label>





